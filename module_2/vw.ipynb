{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import stem\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing contents\n",
    "\n",
    "def clean_tags(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_text(text, mode):\n",
    "    assert mode in ('lemm', 'stem')\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^а-яА-Яa-zA-Z]\", \" \", text)\n",
    "    \n",
    "    if mode == 'lemm':\n",
    "        tokens = mystem.lemmatize(text)\n",
    "        tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "                  and len(token) > 2 \\\n",
    "                  and token.strip() not in punctuation]\n",
    "        \n",
    "    else:\n",
    "        tokens = text.split()\n",
    "        tokens = [stemmer_rus.stem(token) for token in tokens if token not in russian_stopwords\\\n",
    "                  and len(token) > 2 \\\n",
    "                  and token.strip() not in punctuation]\n",
    "        \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_contents(X, mode):\n",
    "    for num, row in X.iterrows():\n",
    "        content = row['content']\n",
    "\n",
    "        content = content.replace('ё', 'е').replace('Ё', 'Е')\n",
    "        \n",
    "        content = clean_tags(content)\n",
    "        content = preprocess_text(content, mode)\n",
    "        \n",
    "        X.at[num, 'content'] = content\n",
    "        print(num)\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare contents for VW\n",
    "\n",
    "def prepare_X_train_vectorized():\n",
    "    with open('in/train_input.vw', \"w\") as f_out:\n",
    "        for num, row in X_train.iterrows():\n",
    "            h = y_train[num]\n",
    "            \n",
    "            line = str(hub_to_label[h]) + \" | \" + \\\n",
    "                ' '.join([str(idx)+':'+str(tf) for idx, tf in zip(W_train[num].indices, W_train[num].data) if tf > 0])\n",
    "            \n",
    "            f_out.write(line + \"\\n\")\n",
    "\n",
    "            \n",
    "def prepare_X_test_vectorized():\n",
    "    with open('in/test_input.vw', \"w\") as f_out:\n",
    "        for num, row in X_test.iterrows():\n",
    "            \n",
    "            line = \"| \" + \\\n",
    "            ' '.join([str(idx)+':'+str(tf) for idx, tf in zip(W_test[num].indices, W_test[num].data) if tf > 0])\n",
    "            \n",
    "            f_out.write(line + \"\\n\")\n",
    "            \n",
    "            \n",
    "def prepare_X_test_glob_vectorized():\n",
    "    with open('in/test_glob_input.vw', \"w\") as f_out:\n",
    "        for num, row in X_test_glob.iterrows():\n",
    "            \n",
    "            line = \"| \" + \\\n",
    "            ' '.join([str(idx)+':'+str(tf) for idx, tf in zip(W_test_glob[num].indices, W_test_glob[num].data) if tf > 0])\n",
    "            \n",
    "            f_out.write(line + \"\\n\")\n",
    "            \n",
    "            \n",
    "def make_vw_input(dtype):\n",
    "    assert dtype in ('train', 'test', 'test_glob')\n",
    "    \n",
    "    if dtype == 'train':\n",
    "        prepare_X_train_vectorized()\n",
    "    elif dtype == 'test':\n",
    "        prepare_X_test_vectorized()\n",
    "    else:\n",
    "        prepare_X_test_glob_vectorized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tags for VW\n",
    "\n",
    "#return tags array\n",
    "def extract_tags(tags_str):\n",
    "    tag_arr = tags_str.split(\"'\")[1::2]\n",
    "    res = []\n",
    "    for tag in tag_arr:\n",
    "        tokens = []\n",
    "        for token in tag.split():\n",
    "            token = re.sub(':', '', token)\n",
    "            token = re.sub(\"[^а-яА-Яa-zA-Z]\", \"\", token)\n",
    "            if len(token) > 2:\n",
    "                tokens.append(token)\n",
    "        new_tag = '_'.join( tokens )\n",
    "        \n",
    "        if len(new_tag) > 2:\n",
    "            res.append(new_tag)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_add_str(X, i):\n",
    "    tags_str = X['tags'][i]\n",
    "    string_to_add = ' ' + ' '.join([tag + ':' + str(tag_weight) for tag in extract_tags(tags_str)])\n",
    "    return string_to_add\n",
    "\n",
    "\n",
    "def add_tags(dtype):\n",
    "    assert dtype in ('train', 'test', 'test_glob')\n",
    "    \n",
    "    if dtype == 'train':\n",
    "        file_name = 'in/train_input.vw'\n",
    "        X = X_train\n",
    "    elif dtype == 'test':\n",
    "        file_name = 'in/test_input.vw'\n",
    "        X = X_test\n",
    "    else:\n",
    "        file_name = 'in/test_glob_input.vw'\n",
    "        X = X_test_glob\n",
    "        \n",
    "    \n",
    "    with open(file_name, 'rt') as fp:\n",
    "        lines = [''.join([line.strip(), get_add_str(X, i), '\\n']) for i, line in enumerate(fp.readlines())]\n",
    "\n",
    "    with open(file_name + '_tagged', 'wt') as fp:\n",
    "        fp.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a lot of time, use prediction_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('data/train_set.csv')\n",
    "X_test_glob = pd.read_csv('data/test_set.csv')\n",
    "\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "tmp_train = preprocess_contents(X_train, 'lemm')\n",
    "tmp_train.to_csv('data/train_set_preproc.csv', encoding='utf8')\n",
    "\n",
    "tmp_test_glob = preprocess_contents(X_test, 'lemm')\n",
    "tmp_test_glob.to_csv('data/test_set_preproc.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "X_train = pd.read_csv('data/train_set_preproc.csv')\n",
    "X_test_glob = pd.read_csv('data/test_set_preproc.csv')\n",
    "\n",
    "#shuffle\n",
    "X_train = X_train.sample(frac=1, random_state=2281488).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop_duplicates(subset='title').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop_duplicates(subset='content').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.dropna(subset=['content']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN in test set\n",
    "X_test_glob['content'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train.hub.values\n",
    "X_train = X_train.drop(\"hub\", 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_to_label = {}\n",
    "label_to_hub = {}\n",
    "for i, hub in enumerate(set(y_train)):\n",
    "    hub_to_label[hub] = i+1\n",
    "    label_to_hub[i+1] = hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = X_train['content'].values\n",
    "texts_test_glob = X_test_glob['content'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.9 s, sys: 396 ms, total: 28.3 s\n",
      "Wall time: 28.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, smooth_idf=True, use_idf=True, stop_words=None)\n",
    "\n",
    "W_train = vectorizer.fit_transform(texts_train)\n",
    "W_test_glob = vectorizer.transform(texts_test_glob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make files for VW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 580 ms, total: 1min 3s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "make_vw_input('train')\n",
    "make_vw_input('test_glob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_weight = 0.5\n",
    "    \n",
    "add_tags('train')\n",
    "add_tags('test_glob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'cache.q': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm cache.q\n",
    "    \n",
    "!vw --oaa 20 -d in/train_input.vw_tagged -f in/vw.model --loss_function logistic \\\n",
    "\\\n",
    "--learning_rate=0.14 --quiet \\\n",
    "\\\n",
    "--passes 10 --cache_file cache.q --ngram 1 --holdout_off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vw -t -i in/vw.model in/test_glob_input.vw_tagged -p out/predict.vw --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "with open(\"out/predict.vw\") as f_in:\n",
    "    for line in f_in:\n",
    "        y_pred.append(int(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out/sample_submission.csv', \"w\") as f_out:\n",
    "    for label_num in y_pred:\n",
    "        hub_pred = label_to_hub[label_num]\n",
    "        f_out.write(hub_pred  + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "with open('out/sample_submission.csv') as f_in:\n",
    "    for label_num in f_in:\n",
    "        y_pred.append(label_num.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide this\n",
    "\n",
    "y_test = []\n",
    "with open(\"data/label_test.csv\") as f_in:\n",
    "    for label_num in f_in:\n",
    "        y_test.append(label_num.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score ~ 0.67...0.71"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
